# ğŸ“ llm_agent.py (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini API)
class GeminiAnswerAgent:
    def __init__(self, model):
        self.model = model

    def answer_question(self, question, chunks):
        for chunk in chunks[:3]:  # Ø¥Ø±Ø³Ø§Ù„ Ø£ÙˆÙ„ 3 Ù‚Ø·Ø¹ ÙÙ‚Ø·
            prompt = f"""
            ğŸ“„ Ù…Ø­ØªÙˆÙ‰ PDF:
            {chunk}

            â“ Ø§Ù„Ø³Ø¤Ø§Ù„:
            {question}
            """
            try:
                response = self.model.generate_content(prompt)
                answer = response.text.strip()
                if answer:
                    return answer
            except Exception as e:
                continue
        return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù."
